{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bd611c-42f7-41f2-873f-60ac2e78efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3af0a7d-b697-4238-8527-e05e59cc7c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4382f735-d01b-4cf3-9aae-464a62589379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "class GenerationMode(ExplicitEnum):\n",
    "    \"\"\"\n",
    "    Possible generation modes, downstream of the [`~generation.GenerationMixin.generate`] method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Non-beam methods\n",
    "    CONTRASTIVE_SEARCH = \"contrastive_search\"\n",
    "    GREEDY_SEARCH = \"greedy_search\"\n",
    "    SAMPLE = \"sample\"\n",
    "    ASSISTED_GENERATION = \"assisted_generation\"\n",
    "    # Beam methods\n",
    "    BEAM_SEARCH = \"beam_search\"\n",
    "    BEAM_SAMPLE = \"beam_sample\"\n",
    "    CONSTRAINED_BEAM_SEARCH = \"constrained_beam_search\"\n",
    "    GROUP_BEAM_SEARCH = \"group_beam_search\"\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "cddd5828-c5d8-4dda-9b29-9d78112349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_new(\n",
    "    self,\n",
    "    inputs: Optional[torch.Tensor] = None,\n",
    "    generation_config: Optional[GenerationConfig] = None,\n",
    "    logits_processor: Optional[LogitsProcessorList] = None,\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "    synced_gpus: Optional[bool] = None,\n",
    "    assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
    "    streamer: Optional[\"BaseStreamer\"] = None,\n",
    "    negative_prompt_ids: Optional[torch.Tensor] = None,\n",
    "    negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "\n",
    "    # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n",
    "\n",
    "    # 2. Set generation parameters if not already defined\n",
    "\n",
    "\n",
    "    # 4. Define other model kwargs\n",
    "    # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
    "    # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
    "\n",
    "       \n",
    "    # 11. run greedy search\n",
    "    \n",
    "    return self.greedy_search(\n",
    "        inputs,\n",
    "        # logits_processor=logits_processor,\n",
    "        stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=461)]),\n",
    "        pad_token_id=0,\n",
    "        eos_token_id=2,\n",
    "        # output_scores=generation_config.output_scores,\n",
    "        return_dict_in_generate=True,\n",
    "        # synced_gpus=synced_gpus,\n",
    "        # streamer=streamer,\n",
    "        # use_cache=True,\n",
    "        # **model_kwargs,\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=4):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_search_new(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        # print(\"IN NEW GREEDY\")\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            \n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, 3)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            # Find and print the newly added text\n",
    "            if updated_text != current_text:\n",
    "                new_text = updated_text[len(current_text):]\n",
    "                # color = GREEN if len(valid_tokens[0]) > 1 else RED\n",
    "                # print(f\"{color}{UNDERLINE}{new_text}{RESET}\", end='')\n",
    "                if len(valid_tokens[0]) > 1:\n",
    "                    color = COLORS[current_color_index]\n",
    "                    print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                    # Update color for next generation\n",
    "                    current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                else:\n",
    "                    print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "0687e122-4aef-4bfd-b93f-bd89ca3062f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor([[13,13,13,12,1,2,3,12,19]], device='cuda:1')\n",
    "# candidate_pred_tokens = find_candidate_pred_tokens(input_ids, 3)\n",
    "# print(candidate_pred_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f22a6f0a-0da1-42c5-b2b8-49f0367e91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate_new = generate_new.__get__(model, MistralForCausalLM)\n",
    "model.greedy_search_new = greedy_search_new.__get__(model, MistralForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "cd02d231-9652-439f-9928-5c18593480a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = \"\"\"# Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering\n",
    "\n",
    "# Inderjeet Nair*1 , Shwetha Somasundaram*\n",
    "\n",
    "## 2 , Apoorv Saxena2 , Koustava Goswami2\n",
    "1University of Michigan, Ann Arbor, MI\n",
    "2Adobe Research, India\n",
    "inair@umich.edu\n",
    "{shsomasu,apoorvs,koustavag}@adobe.com\n",
    "\n",
    "## Abstract\n",
    "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain 99.6% of the best zero-shot approach’s performance, while processing only 26% of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with self-ask reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just ≈ 4% short of zero-shot performance using gold evidence.\n",
    "\n",
    "# 1 Introduction\n",
    "Long Document Question Answering (LDQA) is a complex task that involves locating relevant evidence from lengthy documents to provide accurate answers to specific questions ((<>)Dasigi et al., (<>)2021). LDQA is challenging for the following reasons - a) Long documents often exceed the maximum token limit of existing transformer-based Pretrained Language Models (PLMs) ((<>)Devlin et al., (<>)2019; (<>)Liu (<>)et al., (<>)2019; (<>)Lewis et al., (<>)2020; (<>)Raffel et al., (<>)2020), posing a challenge in directly processing their content to extract pertinent information ((<>)Dong et al., (<>)2023). b) The information required to answer a question is often dispersed across different sections or paragraphs within the document which may require sophisticated reasoning process to identify and extract the relevant information ((<>)Nie et al., (<>)2022). c) Processing the entire document to find answers can be computationally expensive and inefficient ((<>)Dong et al., (<>)2023).\n",
    "* Equal contribution\n",
    "1 Work done at Adobe Research, India\n",
    "One popular approach for LDQA is the retrieve-then-read method ((<>)Zheng et al., (<>)2020; (<>)Gong et al., (<>)2020; (<>)Nie et al., (<>)2022; (<>)Ainslie et al., (<>)2020, (<>)2023), where relevant paragraphs are retrieved from the document to provide the answer. A major drawback of existing works is reliance on supervised fine-tuning for the evidence selection phase, exhibiting poor generalization on out-of-distribution data ((<>)Thakur et al., (<>)2021).\n",
    "Given the remarkable few-shot/zero-shot performance and enhanced generalization capabilities demonstrated by Large Language Models (LLMs) across various Natural Language Generation and Understanding tasks ((<>)Brown et al., (<>)2020; (<>)Chen (<>)et al., (<>)2021; (<>)Rae et al., (<>)2022; (<>)Hoffmann et al., (<>)2022; (<>)Chowdhery et al., (<>)2022), we investigate the potential of leveraging these LLMs for zero-shot evidence retrieval. Notably, LLMs that have been instruction fine-tuned ((<>)Wei et al., (<>)2022a; (<>)Chung et al., (<>)2022) or trained using Reinforcement Learning with Human Feedback ((<>)Bai et al., (<>)2022; (<>)Ouyang (<>)et al., (<>)2022) exhibit exceptional generalization performance even on unseen tasks ((<>)Ouyang et al., (<>)2022; (<>)Min et al., (<>)2022; (<>)OpenAI, (<>)2023). Thus, we explore the feasibility of utilizing LLMs for zero-shot evidence retrieval. However, LLMs, which are based on transformer architecture ((<>)Vaswani (<>)et al., (<>)2017), are limited by their context length and suffer from expensive inference times that increase quadratically with the number of tokens in the input. Additionally, utilizing enterprise LLM solutions such as OpenAI’s gpt-3.5-turbo, text-davinci-003, gpt-4, etc.1 (<>)to process an entire long document without optimizations would incur significant monetary costs. This highlights the need for an LLM-based evidence retrieval solution that can achieve faster and more cost-effective inference by selectively processing relevant portions of the document, without compromising downstream performance.\n",
    "To overcome these challenges, we harness the inherent discourse structure commonly present in long documents. This structure encompasses the organization of topics, semantic segments, and information flow, enabling effective information search and knowledge acquisition for question answering. ((<>)Guthrie et al., (<>)1991; (<>)Meyer et al., (<>)1980; (<>)Taylor (<>)and Beach, (<>)1984; (<>)Cao and Wang, (<>)2022; (<>)Dong et al., (<>)2023; (<>)Nair et al., (<>)2023). Utilizing this valuable structure, we construct a condensed representation of the document by replacing the content within each section with a corresponding summary. This condensed representation is then fed to the LLM, enabling efficient processing of tokens while allowing the model to comprehensively analyze the entire input context for identifying relevant sections. Thereafter, the content within each relevant section is further processed by the LLM for fine-grained evidence retrieval. We call our proposed approach D3 (Drilling Down into the Discourse) due to the nature of the solution described above.\n",
    "Our approach undergoes evaluation in two distinct settings: Information Seeking and Multi-hop Reasoning in Question Answering. In the information seeking experiments, our approach retains the best zero-shot state-of-the-art (SoTA) results, while only utilizing 26% of the tokens employed by the SoTA approach. Additionally, we examine the robustness of our model across various document lengths and analyze the number of tokens required and latency for different zero-shot approaches. Moreover, we explore the integration of our approach with other zero-shot techniques within an agent framework designed to break down intricate queries into a sequence of simpler followup queries.\n",
    "1(<https://openai.com/pricing>)https://openai.com/pricing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "5a884adc-fa78-44a7-946d-9f3fa1b0f938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1788"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the strengths?\"\n",
    "prompt = \"[INST] Document:\\n {doc_text} \\n\\n Question: {question} \\n\\n Answer:[/INST]\".format(doc_text=doc_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)\n",
    "\n",
    "len(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "9da99c6e-8a7f-40c8-873f-7cb0ba9fe527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The paper \"Dr\u001b[31milling Down into the Dis\u001b[0m\u001b[32mcourse Structure with LL\u001b[0m\u001b[34mMs for Long Document Question\u001b[0m\u001b[35m Answering\"\u001b[0m presents a novel approach for zero\u001b[31m-shot evidence retrieval\u001b[0m in long document\u001b[32m question answering (\u001b[0mLD\u001b[34mQA) using\u001b[0m large\u001b[35m language models (LLMs\u001b[0m). The proposed\u001b[31m approach,\u001b[0m called D\u001b[32m3,\u001b[0m leverages the disc\u001b[34mourse structure commonly found in\u001b[0m\u001b[35m documents to\u001b[0m create\u001b[31m a condensed representation of\u001b[0m\u001b[32m the document, enabling a\u001b[0m\u001b[34m more comprehensive understanding and analysis\u001b[0m\u001b[35m of relationships between different parts\u001b[0m\u001b[31m. The\u001b[0m approach ret\u001b[32mains \u001b[0m9\u001b[34m9.6% of\u001b[0m\u001b[35m the best zero-shot\u001b[0m approach's performance while processing\u001b[31m only 26%\u001b[0m\u001b[32m of the total tokens used\u001b[0m\u001b[34m by the best approach in\u001b[0m\u001b[35m the information seeking evidence retriev\u001b[0m\u001b[31mal setup. The\u001b[0m paper also shows how the approach can\u001b[32m be combined with a\u001b[0m self\u001b[34m-ask reasoning agent to\u001b[0m\u001b[35m achieve best zero-shot\u001b[0m performance\u001b[31m in complex multi-hop\u001b[0m\u001b[32m question answering, just \u001b[0m\u001b[34m≈ 4% short\u001b[0m\u001b[35m of zero-shot performance\u001b[0m using\u001b[31m gold evidence. The\u001b[0m authors evaluate the approach in two\u001b[32m distinct settings: Information Se\u001b[0m\u001b[34meking and Multi-hop\u001b[0m\u001b[35m Reasoning in Question\u001b[0m\u001b[31m Answering. In\u001b[0m\u001b[32m the information seeking experiments,\u001b[0m the approach ret\u001b[34mains the best zero-\u001b[0m\u001b[35mshot state\u001b[0m\u001b[31m-of-the-\u001b[0m\u001b[32mart (SoTA)\u001b[0m\u001b[34m results while\u001b[0m only\u001b[35m utilizing 26%\u001b[0m\u001b[31m of the tokens\u001b[0m\u001b[32m employed by the SoTA\u001b[0m\u001b[34m approach. The\u001b[0m paper\u001b[35m also expl\u001b[0mores the robust\u001b[31mness of the\u001b[0m model across\u001b[32m various document lengths and analy\u001b[0mzes the number\u001b[34m of tokens required\u001b[0m\u001b[35m and latency for different\u001b[0m\u001b[31m zero-shot approaches.\u001b[0m\n",
      "\n",
      "Total time: 4.692986249923706 seconds\n",
      "Tokens per second: 61.794342569129526 tokens/sec\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n",
    "max_new_tokens = 300\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the output\n",
    "# out = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=0,return_dict_in_generate=True)\n",
    "with torch.no_grad():\n",
    "    out = model.greedy_search_new(inputs.input_ids, \n",
    "                              attention_mask = inputs.attention_mask,\n",
    "                              stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                              use_cache=True, \n",
    "                              pad_token_id=0,\n",
    "                              eos_token_id=2,\n",
    "                              return_dict_in_generate=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "out_text = tokenizer.batch_decode(out.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "# End timing\n",
    "\n",
    "num_tokens_generated = len(out.sequences[0]) - len(inputs['input_ids'][0])\n",
    "\n",
    "# Calculate the duration and time per token\n",
    "total_time = end_time - start_time\n",
    "tokens_per_sec = num_tokens_generated / total_time\n",
    "\n",
    "print(f\"\\n\\nTotal time: {total_time} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_sec} tokens/sec\")\n",
    "print(num_tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "307a85e6-4aa1-4c85-8964-bdb97017e5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] Document:\\n # Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering\\n\\n# Inderjeet Nair*1 , Shwetha Somasundaram*\\n\\n## 2 , Apoorv Saxena2 , Koustava Goswami2\\n1University of Michigan, Ann Arbor, MI\\n2Adobe Research, India\\ninair@umich.edu\\n{shsomasu,apoorvs,koustavag}@adobe.com\\n\\n## Abstract\\nWe address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain 99.6% of the best zero-shot approach’s performance, while processing only 26% of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with self-ask reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just ≈ 4% short of zero-shot performance using gold evidence.\\n\\n# 1 Introduction\\nLong Document Question Answering (LDQA) is a complex task that involves locating relevant evidence from lengthy documents to provide accurate answers to specific questions ((<>)Dasigi et al., (<>)2021). LDQA is challenging for the following reasons - a) Long documents often exceed the maximum token limit of existing transformer-based Pretrained Language Models (PLMs) ((<>)Devlin et al., (<>)2019; (<>)Liu (<>)et al., (<>)2019; (<>)Lewis et al., (<>)2020; (<>)Raffel et al., (<>)2020), posing a challenge in directly processing their content to extract pertinent information ((<>)Dong et al., (<>)2023). b) The information required to answer a question is often dispersed across different sections or paragraphs within the document which may require sophisticated reasoning process to identify and extract the relevant information ((<>)Nie et al., (<>)2022). c) Processing the entire document to find answers can be computationally expensive and inefficient ((<>)Dong et al., (<>)2023).\\n* Equal contribution\\n1 Work done at Adobe Research, India\\nOne popular approach for LDQA is the retrieve-then-read method ((<>)Zheng et al., (<>)2020; (<>)Gong et al., (<>)2020; (<>)Nie et al., (<>)2022; (<>)Ainslie et al., (<>)2020, (<>)2023), where relevant paragraphs are retrieved from the document to provide the answer. A major drawback of existing works is reliance on supervised fine-tuning for the evidence selection phase, exhibiting poor generalization on out-of-distribution data ((<>)Thakur et al., (<>)2021).\\nGiven the remarkable few-shot/zero-shot performance and enhanced generalization capabilities demonstrated by Large Language Models (LLMs) across various Natural Language Generation and Understanding tasks ((<>)Brown et al., (<>)2020; (<>)Chen (<>)et al., (<>)2021; (<>)Rae et al., (<>)2022; (<>)Hoffmann et al., (<>)2022; (<>)Chowdhery et al., (<>)2022), we investigate the potential of leveraging these LLMs for zero-shot evidence retrieval. Notably, LLMs that have been instruction fine-tuned ((<>)Wei et al., (<>)2022a; (<>)Chung et al., (<>)2022) or trained using Reinforcement Learning with Human Feedback ((<>)Bai et al., (<>)2022; (<>)Ouyang (<>)et al., (<>)2022) exhibit exceptional generalization performance even on unseen tasks ((<>)Ouyang et al., (<>)2022; (<>)Min et al., (<>)2022; (<>)OpenAI, (<>)2023). Thus, we explore the feasibility of utilizing LLMs for zero-shot evidence retrieval. However, LLMs, which are based on transformer architecture ((<>)Vaswani (<>)et al., (<>)2017), are limited by their context length and suffer from expensive inference times that increase quadratically with the number of tokens in the input. Additionally, utilizing enterprise LLM solutions such as OpenAI’s gpt-3.5-turbo, text-davinci-003, gpt-4, etc.1 (<>)to process an entire long document without optimizations would incur significant monetary costs. This highlights the need for an LLM-based evidence retrieval solution that can achieve faster and more cost-effective inference by selectively processing relevant portions of the document, without compromising downstream performance.\\nTo overcome these challenges, we harness the inherent discourse structure commonly present in long documents. This structure encompasses the organization of topics, semantic segments, and information flow, enabling effective information search and knowledge acquisition for question answering. ((<>)Guthrie et al., (<>)1991; (<>)Meyer et al., (<>)1980; (<>)Taylor (<>)and Beach, (<>)1984; (<>)Cao and Wang, (<>)2022; (<>)Dong et al., (<>)2023; (<>)Nair et al., (<>)2023). Utilizing this valuable structure, we construct a condensed representation of the document by replacing the content within each section with a corresponding summary. This condensed representation is then fed to the LLM, enabling efficient processing of tokens while allowing the model to comprehensively analyze the entire input context for identifying relevant sections. Thereafter, the content within each relevant section is further processed by the LLM for fine-grained evidence retrieval. We call our proposed approach D3 (Drilling Down into the Discourse) due to the nature of the solution described above.\\nOur approach undergoes evaluation in two distinct settings: Information Seeking and Multi-hop Reasoning in Question Answering. In the information seeking experiments, our approach retains the best zero-shot state-of-the-art (SoTA) results, while only utilizing 26% of the tokens employed by the SoTA approach. Additionally, we examine the robustness of our model across various document lengths and analyze the number of tokens required and latency for different zero-shot approaches. Moreover, we explore the integration of our approach with other zero-shot techniques within an agent framework designed to break down intricate queries into a sequence of simpler followup queries.\\n1(<https://openai.com/pricing>)https://openai.com/pricing\\n \\n\\n Question: What problem are they solving? Answer in 1 sentence. [/INST] The paper presents a suite of techniques that exploit the discourse structure commonly found in documents to enable a more comprehensive understanding and analysis of relationships between different parts, for the task of zero-shot long document evidence retrieval.'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70b353-7da4-4034-b397-95fccee541d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0c64b6e0-5ba1-4ecf-8fbb-a5b699a26a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   733, 16289, 28793,  6927,  3479,   653,   272,  2296,  3248,\n",
       "          297, 28705, 28740,  1407, 28747,    13,   422,  2985,  8317,  8560,\n",
       "          778,   272,  3433, 11987,  3838,  8187,   395, 16704, 16023,   354,\n",
       "         6428, 14873, 22478,  1094,  1616,  2131,    13,    13, 28771,  1756,\n",
       "          263,  2099,   299,   418,   992, 28736, 28740,  1200,  1295, 28727,\n",
       "          761, 28708,  7068,   293,   915,   762, 28736,    13,    13,  1064,\n",
       "        28705, 28750,  1200,   330,  2345,   271, 28728, 26974,  3594, 28750,\n",
       "         1200,   524, 18361,  1750,   420,   385, 28727,  6449, 28750,    13,\n",
       "        28740, 14953,   472,   302, 13642, 28725,  7303,  1010,  3622, 28725,\n",
       "        17808,    13, 28750,  3261,  8898,  7982, 28725,  5558,    13,   262,\n",
       "          992, 28818,   383,   539, 28723, 17765,    13, 28751,   811, 15415,\n",
       "          293, 28718, 28725,   377, 11019, 10296, 28725, 28729, 18361,   494,\n",
       "          357, 28752, 28818,   316,  8898, 28723,   675,    13,    13,  1064,\n",
       "        14372,    13,  2324,  2962,   272,  3638,   302,  5566, 17913,   282,\n",
       "          354,  1043,  3248,  2996, 24402, 28725,   690, 14657,  1195,  1077,\n",
       "         8598, 18438, 28713,  2373,   264,  3248,   298,  4372,   264,  2996,\n",
       "        28723,   816,  6503,   298,  8084,   272,  4598,  2437,   302,  2475,\n",
       "         3842,  4994,   325,  5292, 16023, 28731,   297,   272,  3638,   302,\n",
       "         6129, 28733,  7063,  1043,  3248,  5566, 17913,   282, 28725,   289,\n",
       "         9988,   298,   652, 26641,  1354, 12713,  4397,  2673,  4118,   418,\n",
       "        11661,  9796, 28723,  2993, 28725,  5489,   272, 16704, 16023,   541,\n",
       "        23327,  6516,  2758, 25458,   390,  2787, 28725,  5884,  7501,  3248,\n",
       "        28632,   390, 14391,  1659, 24890,   272,  3526,  2758,  1312,  6925,\n",
       "          575,   356,  4286,  1378,   272,   791, 28733, 19447, 24940, 28723,\n",
       "        11302, 28725,  5090, 20525,   272,  2475,  2787,  6491,   541,   297,\n",
       "         1352,  5864,  3633,  1249,  6966, 28725,  6311,   739,  9457,   272,\n",
       "         3293,  3248,   325,   391, 13957,   297,  1352,   699,  1326, 23101,\n",
       "        16972,   395, 18355, 10502,  2301,   737,  5629, 11741, 28809, 28713,\n",
       "          420,  6316,  3090,  1549,   609,  1791,  2962,  1167, 10369, 28725,\n",
       "          478, 19333,   264, 10978,   302,  9804,   369, 12573,   279,   272,\n",
       "         2312,  8306,  4693, 14473,  1419,   297, 10181, 28723,  2463, 28464,\n",
       "          456,  4693, 28725,   478,  2231,   264,  2076,  6616,  9283,   302,\n",
       "          272,  3248, 28725, 25748,   264,   680, 15313,  6399,   304,  5643,\n",
       "          302,  9391,  1444,  1581,  5099, 28723,   816, 16815, 28705, 28774,\n",
       "        28774, 28723, 28784, 28823,   302,   272,  1489,  6129, 28733,  7063,\n",
       "         4431, 28809, 28713,  4397, 28725,  1312,  9457,   865, 28705, 28750,\n",
       "        28784, 28823,   302,   272,  3102, 16246,  1307,   486,   272,  1489,\n",
       "         4431,   297,   272,  1871, 11246,  5566, 17913,   282,  9520, 28723,\n",
       "          816,   835,  1347,   910,   813,  4431,   541,   347,  9837,   395,\n",
       "         1008, 28733,  1152, 24685,  8073,   298,  6619,  1489,  6129, 28733,\n",
       "         7063,  4397,   297,  4630,  6079, 28733, 19463,  2996, 24402, 28725,\n",
       "          776, 28705, 29988, 28705, 28781, 28823,  2485,   302,  6129, 28733,\n",
       "         7063,  4397,  1413,  5014,  5566, 28723,    13,   733, 28748, 16289,\n",
       "        28793,   415,  3830,  8395,   274,  1413,  2475,  3842,  4994,   354,\n",
       "         6129, 28733,  7063,  1043,  3248,  5566, 17913,   282,   304, 15890,\n",
       "          264, 10978,   302,  9804,   369, 12573,   279,  2312,  8306,  4693,\n",
       "          298,  2231,   264,  2076,  6616,  9283,   302,   272,  3248, 28725,\n",
       "        25122, 28705, 28774, 28774, 28723, 28784, 28823,   302,   272,  1489,\n",
       "         4431, 28742, 28713,  4397,  1312,  9457,   865, 28705, 28750, 28784,\n",
       "        28823,   302,   272,  3102, 16246,  1307, 28723,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2], device='cuda:1')"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "84e573b5-bae8-4114-983c-332c81947ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GenerationMode.GREEDY_SEARCH: 'greedy_search'>"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._get_generation_mode(model.generation_config, assistant_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "018d1f48-21e8-4c9f-b1c9-5bbe5b25a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config._flash_attn_2_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "203aa948-eb70-41f5-bfbd-32d23e7de81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def autoregressive_decode(model, input_ids, max_length):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model.forward(input_ids=input_ids, use_cache=False)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    tokens_per_second = max_length / total_time\n",
    "\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def autoregressive_decode_with_cache(model, input_ids, max_length):\n",
    "    model.eval()\n",
    "    past_key_values = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        outputs = model.forward(input_ids=input_ids, \n",
    "                                past_key_values=past_key_values, \n",
    "                                use_cache=True,\n",
    "                                return_dict=True)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        past_key_values = outputs.past_key_values\n",
    "        print(past_key_values[0][0].shape[2])\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    tokens_per_second = max_length / total_time\n",
    "\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b0de71af-bc20-46c9-aa53-73cb8c75ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.51 seconds\n",
      "Tokens per second: 19.78\n"
     ]
    }
   ],
   "source": [
    "generated_ids = autoregressive_decode(model, inputs['input_ids'], max_length=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "5cc65968-d535-4070-b693-b1f59281e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "863\n",
      "1296\n",
      "1730\n",
      "2165\n",
      "2601\n",
      "3038\n",
      "3476\n",
      "3915\n",
      "4355\n",
      "Total time: 0.83 seconds\n",
      "Tokens per second: 12.07\n"
     ]
    }
   ],
   "source": [
    "generated_ids = autoregressive_decode_with_cache(model, inputs['input_ids'], max_length=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60af7bac-9adf-42bc-ad05-8c8615b06332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def prepare_inputs_for_generation(\n",
      "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
      "    ):\n",
      "        # Omit tokens covered by past_key_values\n",
      "        if past_key_values:\n",
      "            past_length = past_key_values[0][0].shape[2]\n",
      "\n",
      "            # Some generation methods already pass only the last input ID\n",
      "            if input_ids.shape[1] > past_length:\n",
      "                remove_prefix_length = past_length\n",
      "            else:\n",
      "                # Default to old behavior: keep only final ID\n",
      "                remove_prefix_length = input_ids.shape[1] - 1\n",
      "\n",
      "            input_ids = input_ids[:, remove_prefix_length:]\n",
      "\n",
      "        position_ids = kwargs.get(\"position_ids\", None)\n",
      "        if attention_mask is not None and position_ids is None:\n",
      "            # create position_ids on the fly for batch generation\n",
      "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
      "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
      "            if past_key_values:\n",
      "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
      "\n",
      "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
      "        if inputs_embeds is not None and past_key_values is None:\n",
      "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
      "        else:\n",
      "            model_inputs = {\"input_ids\": input_ids}\n",
      "\n",
      "        model_inputs.update(\n",
      "            {\n",
      "                \"position_ids\": position_ids,\n",
      "                \"past_key_values\": past_key_values,\n",
      "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
      "                \"attention_mask\": attention_mask,\n",
      "            }\n",
      "        )\n",
      "        return model_inputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# Assuming 'model' is your model instance\n",
    "print(inspect.getsource(model.prepare_inputs_for_generation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d8fb0c94-8de1-4197-9b9d-8bf84ab2c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def greedy_search(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor,\n",
      "        logits_processor: Optional[LogitsProcessorList] = None,\n",
      "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
      "        max_length: Optional[int] = None,\n",
      "        pad_token_id: Optional[int] = None,\n",
      "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        output_scores: Optional[bool] = None,\n",
      "        return_dict_in_generate: Optional[bool] = None,\n",
      "        synced_gpus: bool = False,\n",
      "        streamer: Optional[\"BaseStreamer\"] = None,\n",
      "        **model_kwargs,\n",
      "    ) -> Union[GreedySearchOutput, torch.LongTensor]:\n",
      "        r\"\"\"\n",
      "        Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be\n",
      "        used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        In most cases, you do not need to call [`~generation.GenerationMixin.greedy_search`] directly. Use generate()\n",
      "        instead. For an overview of generation strategies and code examples, check the [following\n",
      "        guide](../generation_strategies).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "\n",
      "        Parameters:\n",
      "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "                The sequence used as a prompt for the generation.\n",
      "            logits_processor (`LogitsProcessorList`, *optional*):\n",
      "                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      "                used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      "                used to tell if the generation loop should stop.\n",
      "\n",
      "            max_length (`int`, *optional*, defaults to 20):\n",
      "                **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      "                tokens. The maximum length of the sequence to be generated.\n",
      "            pad_token_id (`int`, *optional*):\n",
      "                The id of the *padding* token.\n",
      "            eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "                The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "            output_attentions (`bool`, *optional*, defaults to `False`):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more details.\n",
      "            output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      "                for more details.\n",
      "            output_scores (`bool`, *optional*, defaults to `False`):\n",
      "                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      "            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "            synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      "            streamer (`BaseStreamer`, *optional*):\n",
      "                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "            model_kwargs:\n",
      "                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
      "                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      "\n",
      "        Return:\n",
      "            [`~generation.GreedySearchDecoderOnlyOutput`], [`~generation.GreedySearchEncoderDecoderOutput`] or\n",
      "            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      "            [`~generation.GreedySearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      "            `return_dict_in_generate=True` or a [`~generation.GreedySearchEncoderDecoderOutput`] if\n",
      "            `model.config.is_encoder_decoder=True`.\n",
      "\n",
      "        Examples:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import (\n",
      "        ...     AutoTokenizer,\n",
      "        ...     AutoModelForCausalLM,\n",
      "        ...     LogitsProcessorList,\n",
      "        ...     MinLengthLogitsProcessor,\n",
      "        ...     StoppingCriteriaList,\n",
      "        ...     MaxLengthCriteria,\n",
      "        ... )\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "\n",
      "        >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
      "        >>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
      "\n",
      "        >>> input_prompt = \"It might be possible to\"\n",
      "        >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      "\n",
      "        >>> # instantiate logits processors\n",
      "        >>> logits_processor = LogitsProcessorList(\n",
      "        ...     [\n",
      "        ...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
      "        ...     ]\n",
      "        ... )\n",
      "        >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
      "\n",
      "        >>> outputs = model.greedy_search(\n",
      "        ...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
      "        ... )\n",
      "\n",
      "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "        [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
      "        ```\"\"\"\n",
      "        # init values\n",
      "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
      "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
      "        if max_length is not None:\n",
      "            warnings.warn(\n",
      "                \"`max_length` is deprecated in this function, use\"\n",
      "                \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
      "                UserWarning,\n",
      "            )\n",
      "            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
      "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
      "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
      "        if isinstance(eos_token_id, int):\n",
      "            eos_token_id = [eos_token_id]\n",
      "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
      "        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n",
      "        output_attentions = (\n",
      "            output_attentions if output_attentions is not None else self.generation_config.output_attentions\n",
      "        )\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n",
      "        )\n",
      "        return_dict_in_generate = (\n",
      "            return_dict_in_generate\n",
      "            if return_dict_in_generate is not None\n",
      "            else self.generation_config.return_dict_in_generate\n",
      "        )\n",
      "\n",
      "        # init attention / hidden states / scores tuples\n",
      "        scores = () if (return_dict_in_generate and output_scores) else None\n",
      "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
      "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
      "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
      "\n",
      "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
      "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
      "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
      "            encoder_hidden_states = (\n",
      "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
      "            )\n",
      "\n",
      "        # keep track of which sequences are already finished\n",
      "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n",
      "\n",
      "        this_peer_finished = False  # used by synced_gpus only\n",
      "        while True:\n",
      "            if synced_gpus:\n",
      "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
      "                # The following logic allows an early break if all peers finished generating their sequence\n",
      "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
      "                # send 0.0 if we finished, 1.0 otherwise\n",
      "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
      "                # did all peers finish? the reduced sum will be 0.0 then\n",
      "                if this_peer_finished_flag.item() == 0.0:\n",
      "                    break\n",
      "\n",
      "            # prepare model inputs\n",
      "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
      "\n",
      "            # forward pass to get next token\n",
      "            outputs = self(\n",
      "                **model_inputs,\n",
      "                return_dict=True,\n",
      "                output_attentions=output_attentions,\n",
      "                output_hidden_states=output_hidden_states,\n",
      "            )\n",
      "\n",
      "            if synced_gpus and this_peer_finished:\n",
      "                continue  # don't waste resources running the code we don't need\n",
      "\n",
      "            next_token_logits = outputs.logits[:, -1, :]\n",
      "\n",
      "            # pre-process distribution\n",
      "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
      "\n",
      "            # Store scores, attentions and hidden_states when required\n",
      "            if return_dict_in_generate:\n",
      "                if output_scores:\n",
      "                    scores += (next_tokens_scores,)\n",
      "                if output_attentions:\n",
      "                    decoder_attentions += (\n",
      "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
      "                    )\n",
      "                    if self.config.is_encoder_decoder:\n",
      "                        cross_attentions += (outputs.cross_attentions,)\n",
      "\n",
      "                if output_hidden_states:\n",
      "                    decoder_hidden_states += (\n",
      "                        (outputs.decoder_hidden_states,)\n",
      "                        if self.config.is_encoder_decoder\n",
      "                        else (outputs.hidden_states,)\n",
      "                    )\n",
      "\n",
      "            # argmax\n",
      "            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
      "\n",
      "            # finished sentences should have their next token be a padding token\n",
      "            if eos_token_id is not None:\n",
      "                if pad_token_id is None:\n",
      "                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
      "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
      "\n",
      "            # update generated ids, model inputs, and length for next step\n",
      "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
      "            if streamer is not None:\n",
      "                streamer.put(next_tokens.cpu())\n",
      "            model_kwargs = self._update_model_kwargs_for_generation(\n",
      "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
      "            )\n",
      "\n",
      "            # if eos_token was found in one sentence, set sentence to finished\n",
      "            if eos_token_id_tensor is not None:\n",
      "                unfinished_sequences = unfinished_sequences.mul(\n",
      "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
      "                )\n",
      "\n",
      "                # stop when each sentence is finished\n",
      "                if unfinished_sequences.max() == 0:\n",
      "                    this_peer_finished = True\n",
      "\n",
      "            # stop if we exceed the maximum length\n",
      "            if stopping_criteria(input_ids, scores):\n",
      "                this_peer_finished = True\n",
      "\n",
      "            if this_peer_finished and not synced_gpus:\n",
      "                break\n",
      "\n",
      "        if streamer is not None:\n",
      "            streamer.end()\n",
      "\n",
      "        if return_dict_in_generate:\n",
      "            if self.config.is_encoder_decoder:\n",
      "                return GreedySearchEncoderDecoderOutput(\n",
      "                    sequences=input_ids,\n",
      "                    scores=scores,\n",
      "                    encoder_attentions=encoder_attentions,\n",
      "                    encoder_hidden_states=encoder_hidden_states,\n",
      "                    decoder_attentions=decoder_attentions,\n",
      "                    cross_attentions=cross_attentions,\n",
      "                    decoder_hidden_states=decoder_hidden_states,\n",
      "                )\n",
      "            else:\n",
      "                return GreedySearchDecoderOnlyOutput(\n",
      "                    sequences=input_ids,\n",
      "                    scores=scores,\n",
      "                    attentions=decoder_attentions,\n",
      "                    hidden_states=decoder_hidden_states,\n",
      "                )\n",
      "        else:\n",
      "            return input_ids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.greedy_search))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed9fd6fb-f217-4e3d-a015-e3f9978b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = model.prepare_inputs_for_generation(inputs.input_ids, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "95877df4-ac36-4f29-8d0f-292de5b39821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   733, 16289, 28793,  6927,  3479,   653,   272,  2296,  3248,\n",
       "            297, 28705, 28740,  1407, 28747,    13,   422,  2985,  8317,  8560,\n",
       "            778,   272,  3433, 11987,  3838,  8187,   395, 16704, 16023,   354,\n",
       "           6428, 14873, 22478,  1094,  1616,  2131,    13,    13, 28771,  1756,\n",
       "            263,  2099,   299,   418,   992, 28736, 28740,  1200,  1295, 28727,\n",
       "            761, 28708,  7068,   293,   915,   762, 28736,    13,    13,  1064,\n",
       "          28705, 28750,  1200,   330,  2345,   271, 28728, 26974,  3594, 28750,\n",
       "           1200,   524, 18361,  1750,   420,   385, 28727,  6449, 28750,    13,\n",
       "          28740, 14953,   472,   302, 13642, 28725,  7303,  1010,  3622, 28725,\n",
       "          17808,    13, 28750,  3261,  8898,  7982, 28725,  5558,    13,   262,\n",
       "            992, 28818,   383,   539, 28723, 17765,    13, 28751,   811, 15415,\n",
       "            293, 28718, 28725,   377, 11019, 10296, 28725, 28729, 18361,   494,\n",
       "            357, 28752, 28818,   316,  8898, 28723,   675,    13,    13,  1064,\n",
       "          14372,    13,  2324,  2962,   272,  3638,   302,  5566, 17913,   282,\n",
       "            354,  1043,  3248,  2996, 24402, 28725,   690, 14657,  1195,  1077,\n",
       "           8598, 18438, 28713,  2373,   264,  3248,   298,  4372,   264,  2996,\n",
       "          28723,   816,  6503,   298,  8084,   272,  4598,  2437,   302,  2475,\n",
       "           3842,  4994,   325,  5292, 16023, 28731,   297,   272,  3638,   302,\n",
       "           6129, 28733,  7063,  1043,  3248,  5566, 17913,   282, 28725,   289,\n",
       "           9988,   298,   652, 26641,  1354, 12713,  4397,  2673,  4118,   418,\n",
       "          11661,  9796, 28723,  2993, 28725,  5489,   272, 16704, 16023,   541,\n",
       "          23327,  6516,  2758, 25458,   390,  2787, 28725,  5884,  7501,  3248,\n",
       "          28632,   390, 14391,  1659, 24890,   272,  3526,  2758,  1312,  6925,\n",
       "            575,   356,  4286,  1378,   272,   791, 28733, 19447, 24940, 28723,\n",
       "          11302, 28725,  5090, 20525,   272,  2475,  2787,  6491,   541,   297,\n",
       "           1352,  5864,  3633,  1249,  6966, 28725,  6311,   739,  9457,   272,\n",
       "           3293,  3248,   325,   391, 13957,   297,  1352,   699,  1326, 23101,\n",
       "          16972,   395, 18355, 10502,  2301,   737,  5629, 11741, 28809, 28713,\n",
       "            420,  6316,  3090,  1549,   609,  1791,  2962,  1167, 10369, 28725,\n",
       "            478, 19333,   264, 10978,   302,  9804,   369, 12573,   279,   272,\n",
       "           2312,  8306,  4693, 14473,  1419,   297, 10181, 28723,  2463, 28464,\n",
       "            456,  4693, 28725,   478,  2231,   264,  2076,  6616,  9283,   302,\n",
       "            272,  3248, 28725, 25748,   264,   680, 15313,  6399,   304,  5643,\n",
       "            302,  9391,  1444,  1581,  5099, 28723,   816, 16815, 28705, 28774,\n",
       "          28774, 28723, 28784, 28823,   302,   272,  1489,  6129, 28733,  7063,\n",
       "           4431, 28809, 28713,  4397, 28725,  1312,  9457,   865, 28705, 28750,\n",
       "          28784, 28823,   302,   272,  3102, 16246,  1307,   486,   272,  1489,\n",
       "           4431,   297,   272,  1871, 11246,  5566, 17913,   282,  9520, 28723,\n",
       "            816,   835,  1347,   910,   813,  4431,   541,   347,  9837,   395,\n",
       "           1008, 28733,  1152, 24685,  8073,   298,  6619,  1489,  6129, 28733,\n",
       "           7063,  4397,   297,  4630,  6079, 28733, 19463,  2996, 24402, 28725,\n",
       "            776, 28705, 29988, 28705, 28781, 28823,  2485,   302,  6129, 28733,\n",
       "           7063,  4397,  1413,  5014,  5566, 28723,    13,   733, 28748, 16289,\n",
       "          28793]], device='cuda:1'),\n",
       " 'position_ids': None,\n",
       " 'past_key_values': None,\n",
       " 'use_cache': True,\n",
       " 'attention_mask': None}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b0cc9-d1f7-4f00-abb4-c854bb9a7900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
